{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Reshape, Dense, Dropout, Flatten, Conv2D, UpSampling2D, ReLU, LeakyReLU, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam\n",
    "from keras import initializers\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1000)\n",
    "\n",
    "noise_dimension = 100\n",
    "epochs=100\n",
    "batch_size=128\n",
    "\n",
    "# Optimizer\n",
    "adam = Adam(lr=0.0002, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST images and labels\n",
    "# We do not need test data, therefore we use everything for training\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "X_train = np.concatenate([X_train, X_test])\n",
    "\n",
    "# Normalize data between -1 and 1. \n",
    "# This leads to better results together with tanh activation that normalizing between 0 and 1 with sigmoid activation.\n",
    "X_train = (X_train - 127.5)/127.5\n",
    "X_train = X_train.reshape((-1, 28, 28, 1))\n",
    "print('Shape of training data:', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build generator\n",
    "generator = Sequential()\n",
    "generator.add(Dense(128*7*7, input_dim=noise_dimension))\n",
    "#generator.add(BatchNormalization())\n",
    "generator.add(Activation('relu'))\n",
    "generator.add(Reshape((7, 7, 128)))\n",
    "generator.add(UpSampling2D(size=(2, 2)))\n",
    "generator.add(Conv2D(64, kernel_size=(5, 5), padding='same'))\n",
    "#generator.add(BatchNormalization())\n",
    "generator.add(Activation('relu'))\n",
    "generator.add(UpSampling2D(size=(2, 2)))\n",
    "generator.add(Conv2D(1, kernel_size=(5, 5), padding='same'))\n",
    "#generator.add(BatchNormalization())\n",
    "generator.add(Activation('tanh'))\n",
    "generator.compile(loss='binary_crossentropy', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build discriminator\n",
    "discriminator = Sequential()\n",
    "discriminator.add(Conv2D(64, kernel_size=(5, 5), strides=(2, 2), padding='same', input_shape=(28, 28, 1), kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "discriminator.add(LeakyReLU(0.2))\n",
    "#discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Conv2D(128, kernel_size=(5, 5), strides=(2, 2), padding='same'))\n",
    "discriminator.add(LeakyReLU(0.2))\n",
    "#discriminator.add(Dropout(0.3))\n",
    "discriminator.add(Flatten())\n",
    "discriminator.add(Dense(1))\n",
    "discriminator.add(Activation('sigmoid'))\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build GAN\n",
    "discriminator.trainable = False\n",
    "gan_in = Input(shape=(noise_dimension,))\n",
    "generator_out = generator(gan_in)\n",
    "gan_out = discriminator(generator_out)\n",
    "gan = Model(inputs=gan_in, outputs=gan_out)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_losses = []\n",
    "generator_losses = []\n",
    "\n",
    "def plot_losses():\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(discriminator_losses, label='Discriminator loss')\n",
    "    plt.plot(generator_losses, label='Generator loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotGeneratedImages():\n",
    "    noise = np.random.normal(0, 1, size=[25, noise_dimension])\n",
    "    generated_images = generator.predict(noise)\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    dim = (5, 5)\n",
    "    for i in range(25):\n",
    "        plt.subplot(dim[0], dim[1], i+1)\n",
    "        plt.imshow(generated_images[i,:,:,0], cmap='gray_r')\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Number of epochs to train:', epochs)\n",
    "print('Training with batch size of:', batch_size)\n",
    "print('Total training samples:', X_train.shape[0])\n",
    "\n",
    "num_iterations = int(X_train.shape[0] / batch_size)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print('Epoch %d' % epoch)\n",
    "    \n",
    "    np.random.shuffle(X_train)\n",
    "    for i in tqdm(range(num_iterations)):\n",
    "        \n",
    "        # Create a training batch containing real images to train discriminator.\n",
    "        # We train with two separate mini-batches each containg only real or only generated images.\n",
    "        X_real = X_train[i*batch_size:i*batch_size+batch_size]\n",
    "        Y_real = np.zeros(batch_size)\n",
    "        # One-sided label smoothing\n",
    "        Y_real.fill(0.9)\n",
    "\n",
    "        discriminator.trainable = True\n",
    "        discriminator_loss_real = discriminator.train_on_batch(X_real, Y_real)\n",
    "\n",
    "        # Create a training batch containing generated images to train discriminator\n",
    "        noise = np.random.normal(0, 1, size=[batch_size, noise_dimension])\n",
    "        X_generated = generator.predict(noise)\n",
    "        Y_generated = np.zeros(batch_size)\n",
    "        discriminator_loss_generated = discriminator.train_on_batch(X_generated, Y_generated)\n",
    "        \n",
    "        discriminator_loss = (discriminator_loss_real + discriminator_loss_generated) / 2\n",
    "        \n",
    "        # Create a training batch to train the generator\n",
    "        noise = np.random.normal(0, 1, size=[batch_size, noise_dimension])\n",
    "        Y_generated = np.zeros(batch_size)\n",
    "        # We flip the labeling (generated = real) for training the generator.\n",
    "        # This is because if the discriminator classifies a generated image as real, \n",
    "        # the loss of the generator is low.\n",
    "        Y_generated.fill(0.9)\n",
    "        discriminator.trainable = False\n",
    "        generator_loss = gan.train_on_batch(noise, Y_generated)\n",
    "\n",
    "    # Save loss of last batch from current epoch\n",
    "    discriminator_losses.append(discriminator_loss)\n",
    "    generator_losses.append(generator_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses()\n",
    "plotGeneratedImages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
